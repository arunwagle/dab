{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fb9690f-89f6-43cf-9cf4-8ee879b0046a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c19f9545-3da6-476f-99b7-82a58beefcfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framework specific imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is required as there is no way to pass a libary to DLT right now.\n",
    "sys.path.append(os.path.abspath('../../src'))\n",
    "\n",
    "from framework.udf_utils import *\n",
    "from framework.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06efbcbe-f54d-4ef2-bbf2-0fdd7145cb10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Get task variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afe090db-1775-4e8a-8457-afeacc26f256",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dimension_table = dbutils.jobs.taskValues.get(taskKey = \"setup\", key = \"dimension_table\", debugValue = \"\")\n",
    "dlt_runtime_config_table = dbutils.jobs.taskValues.get(taskKey = \"setup\", key = \"dlt_runtime_config_table\", debugValue = \"\")\n",
    "# source_system = dbutils.jobs.taskValues.get(taskKey = \"setup\", key = \"source_system\", debugValue = \"\")\n",
    "catalog = dbutils.jobs.taskValues.get(taskKey = \"setup\", key = \"catalog\", debugValue = \"\")\n",
    "schema = dbutils.jobs.taskValues.get(taskKey = \"setup\", key = \"schema\", debugValue = \"\")\n",
    "job_id = dbutils.jobs.taskValues.get(taskKey = \"setup\", key = \"job_id\", debugValue = \"\")\n",
    "run_id = dbutils.jobs.taskValues.get(taskKey = \"setup\", key = \"run_id\", debugValue = \"\")\n",
    "# only used for testing\n",
    "table_suffix = dbutils.jobs.taskValues.get(taskKey = \"setup\", key = \"table_suffix\", debugValue = \"\")\n",
    "test_mode = dbutils.jobs.taskValues.get(taskKey = \"setup\", key = \"test_mode\", debugValue = \"\")\n",
    "\n",
    "schema_location = \"/Volumes/edl_dev_ctlg/rawfiles/raw/dlt_pilot/schema\"\n",
    "\n",
    "dlt_metadata_cdc_config = \"dlt_metadata_cdc_config\"\n",
    "\n",
    "#  Possible values: normal, apply_correction\n",
    "scenario_type = dbutils.jobs.taskValues.get(taskKey = \"setup\", key = \"scenario_type\", debugValue = \"\") \n",
    "\n",
    "# Possible values: full_refresh, incremental_refresh, delta_correction, file_correction\n",
    "refresh_type = dbutils.jobs.taskValues.get(taskKey = \"setup\", key = \"refresh_type\", debugValue = \"\")\n",
    "\n",
    "print(f\"job_load_landing_zone_data_notebook::Getting task variables::\\n\"\n",
    "      f\"dimension_table: {dimension_table}\\n\"\n",
    "      f\"dlt_runtime_config_table: {dlt_runtime_config_table}\\n\"\n",
    "      f\"refresh_type: {refresh_type}\"\n",
    "      f\"catalog: {catalog}\\n\"\n",
    "      f\"schema: {schema}\\n\"      \n",
    "      f\"job_id: {job_id}\\n\"\n",
    "      f\"run_id: {run_id}\\n\"\n",
    "      f\"table_suffix: {table_suffix}\"\n",
    "      f\"schema_location: {schema_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66683906-87cd-483b-a2f4-d6ceca1127c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Static variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fd111bc-ab51-46a6-a145-40e24b4b91fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "landing_table_name_prefix = \"dlt_landing\"\n",
    "delta_landing_table_name = f\"dlt_landing_[TABLE_NAME]{table_suffix}\"\n",
    "source_metadata_table = \"dlt_meta_source_config\"\n",
    "# for historical load for DF_Group and DF_GROUP_COUNT\n",
    "default_business_date = datetime(1900, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API: delete_data_from_landing_table\n",
    "This API is called for apply_correction use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf5a6c57-9d4a-4d83-8573-10db97988f66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def delete_data_from_landing_table(p_catalog, p_schema, p_landing_table, p_business_date):\n",
    "    print (f\"Calling delete_data_from_landing_table\")\n",
    "\n",
    "    query = f\"\"\"\n",
    "        Delete from {p_catalog}.{p_schema}.{p_landing_table} where business_date = '{p_business_date}'\n",
    "    \"\"\"\n",
    "    df = spark.sql (query)\n",
    "    return df.first()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dda938c2-a2b9-4b7d-b2bb-e4021772556b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Main Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99515aa4-b62c-4f4c-b8f0-9cdbb7c4dae3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "runtime_params_row = get_runtime_parameters(\n",
    "    spark, catalog, schema, dlt_runtime_config_table, dimension_table\n",
    ")\n",
    "\n",
    "dimension_table = runtime_params_row[\"dimension_table\"]\n",
    "source_params = runtime_params_row[\"source_params\"]  # this is a dict\n",
    "scenario = runtime_params_row[\"scenario\"]\n",
    "\n",
    "volume_path = None\n",
    "file_format = None\n",
    "\n",
    "\n",
    "if scenario == \"apply_correction\" and scenario_type == \"delta_correction\":\n",
    "    print (f\"scenario::{scenario}, scenario_type::{scenario_type}. This use case is for production support where we update the data directly in the landing table\")\n",
    "else:\n",
    "\n",
    "    for key, param_value_map in source_params.items():\n",
    "\n",
    "        print (f\"key:: {key} param_value_map::{param_value_map}\")\n",
    "        # CDM table E.g. DF_GROUP, DF_GROUP_COUNT\n",
    "        target_table = param_value_map[\"target_table\"]\n",
    "        # Facets, VHP \n",
    "        source_system = param_value_map[\"source_system\"]\n",
    "\n",
    "        # business date for which the data is loaded\n",
    "        business_date_as_str = param_value_map[\"business_date\"]\n",
    "\n",
    "        # default business date to use for historical (audit data load)\n",
    "        # business_date_lit = default_business_date\n",
    "        business_date_lit = datetime.strptime(business_date_as_str, \"%Y-%m-%d\")\n",
    "\n",
    "        source_metadata = get_source_metadata(\n",
    "            spark,\n",
    "            catalog,\n",
    "            schema,\n",
    "            dimension_table,\n",
    "            source_metadata_table,\n",
    "            target_table,\n",
    "            business_date_as_str,\n",
    "            scenario,\n",
    "            source_system,\n",
    "            p_test_mode=test_mode\n",
    "        )\n",
    "\n",
    "        # source metadata row\n",
    "        source_metadata_row = source_metadata\n",
    "        # source name (E.g. DF_GROUP_FACETS, DF_GROUP_VHP)\n",
    "        # source = source_metadata_row.source\n",
    "        # source name (E.g. dbo, audit, cdc)\n",
    "        source_schema = source_metadata_row.source_schema\n",
    "    \n",
    "        derived_input_table_list = [\n",
    "            item.strip() for item in source_metadata_row.derived_input_tables.split(\",\")\n",
    "        ]\n",
    "        # business_date_column_name is specifically used for historical load. This column will be mapped to business_date\n",
    "        business_date_column_name = source_metadata_row.business_date_column_to_load\n",
    "\n",
    "        # This will passed from the runtime_params if scenario = 'apply_correction' or 'apply_test'\n",
    "        if \"source_details\" in param_value_map and param_value_map[\"source_details\"] != \"\":\n",
    "            volume_path = param_value_map[\"source_details\"]\n",
    "        else:\n",
    "            volume_path = source_metadata_row.source_details\n",
    "\n",
    "        if \"file_format\" in param_value_map and param_value_map[\"file_format\"] != \"\":\n",
    "            file_format = param_value_map[\"file_format\"]\n",
    "        else:\n",
    "            file_format = source_metadata_row.file_format\n",
    "\n",
    "        scd_row = get_scd_attributes(\n",
    "            p_spark=spark,\n",
    "            p_catalog=catalog,\n",
    "            p_schema=schema,\n",
    "            p_dlt_metadata_cdc_config=dlt_metadata_cdc_config,\n",
    "            p_target_table=target_table,\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"load_landing_zone::for \\nsource_schema::{source_schema} \\nbusiness_date::{business_date_lit}\\nvolume_path::{volume_path}\"\n",
    "        )\n",
    "\n",
    "        # Read data from source\n",
    "        header_option = \"true\" if file_format == \"csv\" else \"false\"\n",
    "        print(f\"file_format:{file_format}, header_option:{header_option}\")\n",
    "        df = (\n",
    "            spark.read.format(\"cloudFiles\")\n",
    "            .format(file_format)\n",
    "            .option(\"header\", header_option)\n",
    "            .load(volume_path)\n",
    "            .withColumn(\"source\", F.lit(\"Not Used\"))\n",
    "            .withColumn(\"source_schema\", F.lit(source_schema))\n",
    "        )\n",
    "\n",
    "        #  Process each CDM entity \n",
    "        for index, input_table_name in enumerate(derived_input_table_list):\n",
    "            \n",
    "            landing_table_name = delta_landing_table_name.replace(\n",
    "                \"[TABLE_NAME]\", input_table_name\n",
    "            )\n",
    "            print(f\"landing_table_name:{landing_table_name}\")\n",
    "                    \n",
    "            # Legacy source has some columns with __$.\n",
    "            # This step to cast is required as otherwise some fields (__$lstart etc) throw error as they are not casted to string\n",
    "            for col_name in df.columns:\n",
    "                if col_name.startswith(\"__$\"):\n",
    "                    df = df.withColumn(col_name, F.col(col_name).cast(\"string\"))\n",
    "\n",
    "            # if the source does not send CDC information (E.g. DF_GROUP_COUNT), then we have to compute inserts, udpates and deletes\n",
    "            if \"__$operation\" not in df.columns:\n",
    "                # read target landing table  \n",
    "                df = df.withColumn(\"__$operation\", F.lit(\"5\"))  \n",
    "                # compute delete            \n",
    "                df = compute_deletes_from_snapshots(spark, scenario, catalog, schema, landing_table_name, df, scd_row)\n",
    "                \n",
    "            #  While loading historical data, this is useful to get the business date from that column value.\n",
    "            if business_date_column_name and business_date_column_name.strip():\n",
    "                df = df.withColumn(\n",
    "                    \"business_date\", F.col(business_date_column_name).cast(\"date\")\n",
    "                )\n",
    "            else:\n",
    "                df = df.withColumn(\n",
    "                    \"business_date\", F.lit(business_date_lit).cast(\"date\")\n",
    "                )\n",
    "\n",
    "            # Prod support use case: Handle scenario='apply_correction'\n",
    "            # Delete the existing data in the landing table \n",
    "            if scenario == \"apply_correction\" and scenario_type == \"file_correction\":\n",
    "                delete_result = delete_data_from_landing_table (catalog, schema, landing_table_name, business_date_lit)\n",
    "                print (f\" delete_data_from_landing_table for delete_result::{delete_result} for scenario:{scenario}\")\n",
    "\n",
    "            print (f\"refresh_type::{refresh_type}\")\n",
    "            if refresh_type == \"incremental_refresh\":\n",
    "                (\n",
    "                    df.write.format(\"delta\")\n",
    "                    .mode(\"append\")\n",
    "                    .option(\"mergeSchema\", \"true\")\n",
    "                    .option(\"delta.columnMapping.mode\", \"name\")\n",
    "                    .option(\"delta.enableChangeDataFeed\", \"false\")\n",
    "                    .saveAsTable(f\"{catalog}.{schema}.{landing_table_name}\")\n",
    "                )\n",
    "            # CAUTION: This condition should ony be used when we want to do a full refresh of the pipeline. \n",
    "            # This will reset the DLT pipeline data. Only used first time the data load starts , historical data load.\n",
    "            elif refresh_type == \"full_refresh\":\n",
    "                (\n",
    "                    df.write.format(\"delta\")\n",
    "                    .mode(\"overwrite\")\n",
    "                    .option(\"mergeSchema\", \"true\")\n",
    "                    .option(\"delta.columnMapping.mode\", \"name\")\n",
    "                    .option(\"delta.enableChangeDataFeed\", \"false\")\n",
    "                    .saveAsTable(f\"{catalog}.{schema}.{landing_table_name}\")\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "job_load_landing_zone_data_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
