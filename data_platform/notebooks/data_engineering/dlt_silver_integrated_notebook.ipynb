{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db5de1e1-1ca0-4ba1-a60b-7b43a4ead70a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Silver DLT Pipeline Task \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "938c38a5-1165-434b-b9eb-186086145e23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Global Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d271869-2be8-4143-894c-ac1ba37ebe7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "# Import the required modules from PySpark SQL\n",
    "from pyspark.sql import SparkSession, Row\n",
    "# from pyspark.sql.functions import col, lit, expr, when, upper, create_map, row_number\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DoubleType, TimestampType, DateType, BooleanType, MapType, ShortType\n",
    "# from pyspark.sql.utils import AnalysisException\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d124405b-b53e-44f1-bba0-90a429f4cbac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Include folder path to include framework python files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f166a776-85c6-4954-8c3b-f9fdea8d1c89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Auto merges schema mismatch issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e6a53e7-d575-4def-831e-b696dbec8844",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06ed111c-0540-4d93-af3a-e370062a2434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parameters required for this DLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3a13294-cdcf-422f-a425-6cc58bda3f6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "     \n",
    "    1. Fetch the run_date based on the dimension_table and status = 'IN_PROGRESS' from the workflow_runtime_audit table.\n",
    "    2. Only 1 DLT pipeline per dimension_table can be run at a time, so you should always see 1 workflow IN_PROGRESS for any given source and run_date\n",
    "    # E.g. DF_GROUP_FACETS, DF_GROUP_VHP\n",
    "\"\"\"\n",
    "\n",
    "dimension_table = spark.conf.get(\"dimension_table\")\n",
    "\n",
    "catalog = spark.conf.get(\"catalog\")\n",
    "schema = spark.conf.get(\"schema\")\n",
    "\n",
    "# This parameter is used only for testing purpose, for multiple people working on their own DLT pipleines. This is not used in the production.\n",
    "# Note: When using Databricks Asset Bundle, this can be handled automatically as a configuration\n",
    "table_suffix = spark.conf.get(\"table_suffix\")\n",
    "if table_suffix == \"N/A\":\n",
    "    table_suffix = \"\"\n",
    "else:\n",
    "    # only set for testing or individual developer testing their DLT's\n",
    "    table_suffix = f\"_{table_suffix}\"\n",
    "\n",
    "test_mode = spark.conf.get(\"test_mode\")\n",
    "\n",
    "# This is the time when the DLT pipeline is run. \n",
    "# TODO: Rename to dlt_start_time\n",
    "processing_time = int(datetime.now().timestamp())\n",
    "print(f\"Parameters passed from the DLT pipeline: catalog:{catalog}, schema:{schema}, processing_time::{processing_time}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framework specific imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06290220-0a63-4b48-9f41-6c9738d3718e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# This is required as there is no way to pass a libary to DLT right now.\n",
    "sys.path.append(os.path.abspath('../../src'))\n",
    "\n",
    "from framework.utils import *\n",
    "from framework.dlt_utils import *\n",
    "from framework.udf_utils import *\n",
    "\n",
    "\n",
    "import importlib\n",
    "\n",
    "module_name = f\"framework.{dimension_table.lower()}.{dimension_table.lower()}_silver\"\n",
    "module = importlib.import_module(module_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9f6184e-729b-4a80-8d60-eedfe0652c63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Static Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe9aa788-8558-4596-a314-e062698ad67a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "business_date_format=\"yyyyMMdd\"\n",
    "dlt_scd_sequence_column_name = \"business_date\"\n",
    "# \"loadtimestamp\"\n",
    "# \"processing_time\"\n",
    "\n",
    "dlt_runtime_config_table = \"dlt_runtime_config\"\n",
    "\n",
    "dlt_metadata_source_config = \"dlt_meta_source_config\"\n",
    "dlt_metadata_cdc_config = \"dlt_meta_cdc_config\"\n",
    "dlt_metadata_dq_config  = \"dlt_meta_dq_config\"\n",
    "dlt_metadata_transform_config = \"dlt_meta_transform_config\"\n",
    "\n",
    "dlt_sink_audit_table_name = f\"dlt_dq_audit{table_suffix}\"\n",
    "\n",
    "dlt_stream_landing_table_name = f\"dlt_landing_[TABLE_NAME]{table_suffix}\"\n",
    "dlt_stream_bronze_table_name = f\"dlt_brz_[TABLE_NAME]{table_suffix}\"\n",
    "dlt_stream_silver_dq_table_name = f\"dlt_slv_clean_[TABLE_NAME][STEP_NAME]{table_suffix}\"\n",
    "dlt_stream_silver_transform_table_name = f\"dlt_slv_transform_[TABLE_NAME]{table_suffix}\"\n",
    "dlt_stream_silver_mapped_table_name = f\"dlt_slv_mapped_[TABLE_NAME]{table_suffix}\"\n",
    "dlt_stream_silver_cdc_table_name = f\"dlt_slv_scd_[TABLE_NAME]{table_suffix}\"\n",
    "# dlt_materialized_silver_target_table_name = f\"[TABLE_NAME]{table_suffix}\"\n",
    "# dlt_materialized_gold_target_table_name = f\"[TABLE_NAME]{table_suffix}\"\n",
    "dlt_stream_silver_scd_target_table = f\"dlt_slv_scd_[TABLE_NAME]{table_suffix}\"\n",
    "dlt_stream_silver_target_table = f\"dlt_[TABLE_NAME]{table_suffix}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f71709dc-9343-4b01-bf7b-d8c4417152bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DLT Logic starts here\n",
    "This API  is responsible for creating the DLT pipelines for particular dimension table and target table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db0cd977-ba99-4f6e-b746-7e22f9d41881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_preprocessing_dlt(\n",
    "    p_spark,\n",
    "    p_catalog,\n",
    "    p_schema,\n",
    "    p_dimension_table,\n",
    "    p_target_table,\n",
    "    p_dlt_metadata_source_config,\n",
    "    p_business_date,\n",
    "    p_processing_time,\n",
    "    p_scenario,\n",
    "    p_source_system,\n",
    "    p_test_mode\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs the Delta Live Tables (DLT) pipeline for the specified dimension table and target table,\n",
    "    including Change Data Capture (CDC) processing at the end.\n",
    "\n",
    "    Parameters:\n",
    "    - p_dimension_table (str): The dimension table to process. E.g: D_GROUP\n",
    "    - p_target_table (str): The target table to create or update. DF_GROUP, DF_GROUP_COUNT, DF_GROUP_HISTORY\n",
    "    - p_business_date (str): The business date for the data.\n",
    "\n",
    "    Returns:\n",
    "    - None: This function does not return any value. It processes the DLT pipeline.\n",
    "    \"\"\"\n",
    "    print(\n",
    "        f\"run_dlt:: for dimension_table:{p_dimension_table} and cdm table :{p_target_table} for business_date:{p_business_date} at  processing_time:{p_processing_time}\"\n",
    "    )\n",
    "\n",
    "    # fetch the metadata from the metadata table\n",
    "    source_metadata = get_source_metadata(\n",
    "        p_spark=p_spark,\n",
    "        p_catalog=p_catalog,\n",
    "        p_schema=p_schema,\n",
    "        p_dimension_table=p_dimension_table,\n",
    "        p_source_metadata_table=p_dlt_metadata_source_config,\n",
    "        p_target_table=p_target_table,\n",
    "        p_business_date=p_business_date,\n",
    "        p_scenario=p_scenario,\n",
    "        p_source_system=p_source_system,\n",
    "        p_test_mode = p_test_mode\n",
    "    )\n",
    "\n",
    "    print(f\"run_dlt:: metadata_source_config::{source_metadata}\")\n",
    "    if source_metadata is None:\n",
    "        return\n",
    "    # source metadata row\n",
    "    row = source_metadata\n",
    "    # source name (E.g. DF_GROUP_FACETS, DF_GROUP_VHP)\n",
    "    source = row.source_system \n",
    "    # row.source\n",
    "    # source name (E.g. dbo, audit, cdc)\n",
    "    source_schema = row.source_schema\n",
    "\n",
    "    derived_input_table_list = [\n",
    "        item.strip() for item in row.derived_input_tables.split(\",\")\n",
    "    ]\n",
    "\n",
    "    derived_target_table_list = [\n",
    "        item.strip() for item in row.derived_target_tables.split(\",\")\n",
    "    ]\n",
    "    is_active = row.is_active\n",
    "    print(       \n",
    "        f\"source_schema::{source_schema}\\n\"\n",
    "        f\"derived_input_table_lgenerate_bronzetablesist::{derived_input_table_list}\\n\"\n",
    "        f\"derived_target_table_list::{derived_target_table_list}\"\n",
    "        f\"is_active::{is_active}\"\n",
    "    )\n",
    "\n",
    "    for index, input_table in enumerate(derived_input_table_list):\n",
    "        print(\n",
    "            f\"run_dlt:: for input_table:{input_table}, target_table_list:{derived_target_table_list[index]}\"\n",
    "        )\n",
    "        # actual DF_GROUP, DF_GROUP_COUNT, DF_GROUP_HISTORY\n",
    "        target_table = derived_target_table_list[index].lower()\n",
    "\n",
    "        # Step 1: Handle Bronze\n",
    "        dlt_source_table = get_table_name(dlt_stream_landing_table_name, input_table)\n",
    "        dlt_target_table = get_table_name(dlt_stream_bronze_table_name, input_table)\n",
    "\n",
    "        # TODO: The code fails if the landing zone delta table does not exist. Create an empty schema table in generate_bronze to handle this use case\n",
    "        # try:\n",
    "        #     spark.table(dlt_source_table)\n",
    "        # except Exception:\n",
    "        #     continue\n",
    "        bronze_table_to_column_map = module.get_bronze_table_to_column_map()\n",
    "\n",
    "        global_cols = get_bronze_table_columns(bronze_table_to_column_map, \"GLOBALS\", \"all\", \"all\", \"global_columns\")\n",
    "        print(f\"generate_bronzetables:: global_cols ::{global_cols}\")\n",
    "        required_cols = get_bronze_table_columns(\n",
    "            bronze_table_to_column_map, source, source_schema, input_table, \"required_columns\"\n",
    "        )\n",
    "        additional_cols = get_bronze_table_columns(\n",
    "            bronze_table_to_column_map, source, source_schema, input_table, \"additional_columns\"\n",
    "        )\n",
    "        print(f\"generate_bronzetables:: additional_cols ::{additional_cols}\")\n",
    "        df = generate_bronzetables(\n",
    "            p_spark=p_spark,\n",
    "            p_catalog=p_catalog,\n",
    "            p_schema=p_schema,\n",
    "            p_dimension_table=p_dimension_table,\n",
    "            p_input_table=input_table,\n",
    "            p_dlt_source_table=dlt_source_table,\n",
    "            p_dlt_target_table=dlt_target_table,\n",
    "            p_source=source,\n",
    "            p_source_schema=source_schema,\n",
    "            p_business_date=p_business_date,\n",
    "            p_business_date_format=business_date_format,\n",
    "            p_processing_time=p_processing_time,\n",
    "            p_global_cols=global_cols,\n",
    "            p_required_cols=required_cols,\n",
    "            p_additional_cols=additional_cols\n",
    "        )\n",
    "\n",
    "        # print(f\"=====%%%%%%%%========bronzetables df::{df.collect()}\")\n",
    "\n",
    "        # Step 3: Data Quality validations\n",
    "        # Step 1: FAIL\n",
    "        dlt_source_table = dlt_target_table\n",
    "        dlt_target_table = get_table_name(\n",
    "            dlt_stream_silver_dq_table_name, input_table, \"fail_step\"\n",
    "        )\n",
    "        generate_dq_table(\n",
    "            p_spark=p_spark,\n",
    "            p_catalog=p_catalog,\n",
    "            p_schema=p_schema,\n",
    "            p_dimension_table=p_dimension_table,\n",
    "            p_source_system = source,\n",
    "            p_target_table=target_table,\n",
    "            p_dlt_source_table=dlt_source_table,\n",
    "            p_dlt_target_table=dlt_target_table,\n",
    "            p_dlt_audit_table=dlt_sink_audit_table_name,\n",
    "            p_dq_type=\"FAIL\",\n",
    "            p_dlt_metadata_dq_config=dlt_metadata_dq_config\n",
    "        )\n",
    "\n",
    "        # Step 2: QUARANTINE\n",
    "        dlt_source_table = dlt_target_table\n",
    "        dlt_target_table = get_table_name(\n",
    "            dlt_stream_silver_dq_table_name, input_table, \"quarantine_step\"\n",
    "        )\n",
    "        generate_dq_table(\n",
    "            p_spark=p_spark,\n",
    "            p_catalog=p_catalog,\n",
    "            p_schema=p_schema,\n",
    "            p_dimension_table=p_dimension_table,\n",
    "            p_source_system = source,\n",
    "            p_target_table=target_table,\n",
    "            p_dlt_source_table=dlt_source_table,\n",
    "            p_dlt_target_table=dlt_target_table,\n",
    "            p_dlt_audit_table=dlt_sink_audit_table_name,\n",
    "            p_dq_type=\"QUARANTINE\",\n",
    "            p_dlt_metadata_dq_config=dlt_metadata_dq_config\n",
    "        )\n",
    "\n",
    "        # Step 3: WARN\n",
    "        dlt_source_table = dlt_target_table\n",
    "        dlt_target_table = get_table_name(\n",
    "            dlt_stream_silver_dq_table_name, input_table, \"warn_step\"\n",
    "        )\n",
    "        generate_dq_table(\n",
    "            p_spark=p_spark,\n",
    "            p_catalog=p_catalog,\n",
    "            p_schema=p_schema,\n",
    "            p_dimension_table=p_dimension_table,\n",
    "            p_source_system = source,\n",
    "            p_target_table=target_table,\n",
    "            p_dlt_source_table=dlt_source_table,\n",
    "            p_dlt_target_table=dlt_target_table,\n",
    "            p_dlt_audit_table=dlt_sink_audit_table_name,\n",
    "            p_dq_type=\"WARN\",\n",
    "            p_dlt_metadata_dq_config=dlt_metadata_dq_config\n",
    "        )\n",
    "\n",
    "        # Apply simple transformation rules.\n",
    "        dlt_source_table = dlt_target_table\n",
    "        dlt_target_table = get_table_name(\n",
    "            dlt_stream_silver_transform_table_name, input_table\n",
    "        )\n",
    "\n",
    "        generate_tranformation_rules_df(\n",
    "            p_spark=p_spark,\n",
    "            p_catalog=p_catalog,\n",
    "            p_schema=p_schema,\n",
    "            p_dimension_table=p_dimension_table,\n",
    "            p_source_system = source,\n",
    "            p_target_table=target_table,\n",
    "            p_dlt_source_table=dlt_source_table,\n",
    "            p_dlt_target_table=dlt_target_table,\n",
    "            p_dlt_metadata_transform_config=dlt_metadata_transform_config\n",
    "        )\n",
    "\n",
    "        dlt_source_table = dlt_target_table\n",
    "        dlt_target_table = get_table_name(\n",
    "            f\"{dlt_stream_silver_mapped_table_name}_{p_source_system.lower()}\", target_table.lower()\n",
    "        )\n",
    "        generate_mapped_tables_by_source_system(\n",
    "            p_spark=p_spark,\n",
    "            p_catalog=p_catalog,\n",
    "            p_schema=p_schema,\n",
    "            p_source=source,\n",
    "            p_dimension_table=p_dimension_table,\n",
    "            p_input_table=input_table,\n",
    "            p_target_table=target_table,\n",
    "            p_dlt_source_table=dlt_source_table,\n",
    "            p_dlt_target_table=dlt_target_table,\n",
    "            p_business_date=p_business_date,\n",
    "            \n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1383758-f5af-4422-a1b3-a669fb0dbf20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_postprocessing_dlt(\n",
    "    p_spark,\n",
    "    p_catalog,\n",
    "    p_schema,\n",
    "    p_dlt_metadata_source_config,\n",
    "    p_dimension_table,\n",
    "    p_sources_metadata,\n",
    "    scenario\n",
    "):\n",
    "    print(\"Starting run_postprocessing_dlt\")\n",
    "    # Final step\n",
    "    target_tables_dict = get_target_tables_for_run(\n",
    "        p_spark,\n",
    "        catalog,\n",
    "        schema,\n",
    "        dlt_metadata_source_config,\n",
    "        dimension_table,\n",
    "        p_sources_metadata,\n",
    "        scenario\n",
    "    )\n",
    "\n",
    "    for target_table, source_system_list in target_tables_dict.items():\n",
    "        print(\n",
    "            f\"run_postprocessing_dlt: processing target_table:{target_table} and source_system_list:{source_system_list}\"\n",
    "        )\n",
    "        dlt_target_table = get_table_name(\n",
    "            dlt_stream_silver_mapped_table_name, target_table.lower()\n",
    "        )\n",
    "        dlt.create_streaming_table(dlt_target_table)\n",
    "\n",
    "        #  Code to generate df tables from multiple source system\n",
    "        for source_system in source_system_list:\n",
    "            dlt_source_table = get_table_name(\n",
    "                f\"{dlt_stream_silver_mapped_table_name}_{source_system.lower()}\",\n",
    "                target_table.lower(),\n",
    "            )\n",
    "            generate_df_append_tables(\n",
    "                p_spark=p_spark,\n",
    "                p_catalog=catalog,\n",
    "                p_schema=schema,\n",
    "                p_dlt_source_table=dlt_source_table,\n",
    "                p_dlt_target_table=dlt_target_table,\n",
    "            )\n",
    "\n",
    "        scd_row = get_scd_attributes(\n",
    "            p_spark=p_spark,\n",
    "            p_catalog=p_catalog,\n",
    "            p_schema=p_schema,\n",
    "            p_dlt_metadata_cdc_config=dlt_metadata_cdc_config,\n",
    "            p_target_table=target_table,\n",
    "        )\n",
    "        print(f\"run_postprocessing_dlt:: scd_row::{scd_row}\")\n",
    "        # For History Load , no CDC\n",
    "        if scd_row is not None:\n",
    "            dlt_source_table = dlt_target_table\n",
    "            dlt_target_table = get_table_name(\n",
    "                f\"{dlt_stream_silver_scd_target_table}\",\n",
    "                target_table.lower(),\n",
    "            )\n",
    "\n",
    "            key_attr = scd_row.key_attr\n",
    "            scd_type = scd_row.scd_type\n",
    "            exclude_columns_list = scd_row.exclude_columns\n",
    "            sequence_column_name = scd_row.sequence_col\n",
    "            df = generate_scd_tables(\n",
    "                p_target_table=target_table,\n",
    "                p_dlt_source_table=dlt_source_table,\n",
    "                p_dlt_target_table=dlt_target_table,\n",
    "                p_dlt_stream_silver_target_table=dlt_stream_silver_target_table,\n",
    "                p_keys=key_attr,\n",
    "                p_seq_col=sequence_column_name,\n",
    "                p_scd_type=scd_type,\n",
    "                p_exclude_column_list=exclude_columns_list\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"run_postprocessing_dlt:: No SCD attributes found for target table::{target_table}. This condition needs to be handled\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cd3a373-4680-49b1-a251-237dd0d93a31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Execution starts here\n",
    "This section initiates the execution of the defined DLT flow.   \n",
    "- Reads the data from dim_group_runtime_metadata, which is set from the ControlM job (run_entity_job for POC)\n",
    "- Runs the DLT pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd4b4319-a741-4dae-9981-1226d05a7fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "runtime_params_row = get_runtime_parameters(\n",
    "    spark, catalog, schema, dlt_runtime_config_table, dimension_table\n",
    ")\n",
    "\n",
    "dimension_table = runtime_params_row[\"dimension_table\"]\n",
    "source_params = runtime_params_row[\"source_params\"]  # this is a dict\n",
    "scenario = runtime_params_row[\"scenario\"]\n",
    "\n",
    "print(f\"\\nüîç Processing Dimension Table: {dimension_table}\")\n",
    "\n",
    "for key, param_value_map in source_params.items():\n",
    "    target_table = param_value_map[\"target_table\"]\n",
    "    business_date = param_value_map[\"business_date\"]\n",
    "    source_system = param_value_map[\"source_system\"]\n",
    "    # load_type = \"incremental\"\n",
    "\n",
    "    # Set to default value for historical load\n",
    "    # if business_date is None or business_date == \"\" or business_date == \"N/A\":\n",
    "    #     business_date = \"1900-01-01\"\n",
    "    #     load_type = \"one_time\"\n",
    "\n",
    "    # Custom logic here\n",
    "    print(\n",
    "        f\"Start: DLT pipeline for : target_table:: {target_table} for business Date: {business_date}\"\n",
    "    )\n",
    "\n",
    "    run_preprocessing_dlt(\n",
    "        spark,\n",
    "        catalog,\n",
    "        schema,\n",
    "        dimension_table,\n",
    "        target_table,\n",
    "        dlt_metadata_source_config,\n",
    "        business_date,\n",
    "        p_processing_time = processing_time,\n",
    "        p_scenario=scenario,\n",
    "        p_source_system=source_system,\n",
    "        p_test_mode=test_mode\n",
    "    )\n",
    "\n",
    "run_postprocessing_dlt(\n",
    "    spark,\n",
    "    catalog,\n",
    "    schema,\n",
    "    dlt_metadata_source_config,\n",
    "    dimension_table,\n",
    "    source_params,\n",
    "    scenario\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "dlt_silver_integrated_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
