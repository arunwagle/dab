{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92bacb93-1b01-4f6a-a527-247beb88e4e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Control-M to pass these values to the Databricks workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6e830d9-309e-472d-bede-74b9b8c81992",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Global imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed7bbeb8-d33f-4621-a2a0-fde5478ef9a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40c98658-e2be-499a-96a7-1d08186c7207",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Dynamic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f4970a3-6954-4beb-beae-93d4c88e7589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Created a secret scope databricks secrets create-scope --scope dlt-pilot-scope\n",
    "# databricks secrets put-secret dlt-pilot-scope api_access_pat_token\n",
    "DATABRICKS_TOKEN = dbutils.secrets.get(scope=\"dlt-pilot-scope\", key=\"api_access_pat_token\")\n",
    "\n",
    "# job id of the DLT workflow\n",
    "job_id = dbutils.widgets.getArgument(\"job_id\")\n",
    "\n",
    "db_workspace_url = dbutils.widgets.getArgument(\"db_workspace_url\")\n",
    "\n",
    "catalog = dbutils.widgets.getArgument(\"catalog\")\n",
    "\n",
    "bronze_schema = dbutils.widgets.getArgument(\"bronze_schema\")\n",
    "\n",
    "dlt_runtime_config_table = \"dlt_runtime_config\"\n",
    "\n",
    "table_suffix = dbutils.widgets.getArgument(\"table_suffix\")\n",
    "if table_suffix == \"N/A\":\n",
    "    table_suffix = \"\"\n",
    "else:\n",
    "    # only set for testing or individual developer testing their DLT's\n",
    "    table_suffix = f\"_{table_suffix}\"\n",
    "\n",
    "selected_dimension = dbutils.widgets.getArgument(\"dimension_table\")\n",
    "selected_scenario = dbutils.widgets.getArgument(\"scenario\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the distinct scenarios and dimensions from the dim_integration_test_scenarios table\n",
    "# df = spark.sql(\"SELECT dimension_table, scenario FROM edl_dev_ctlg.structured.dim_integration_test_scenarios\")\n",
    "# dimensions_list = sorted([row['dimension_table'] for row in df.select(\"dimension_table\").distinct().collect()])\n",
    "# scenario_list = sorted([row['scenario'] for row in df.select(\"scenario\").distinct().collect()])\n",
    "\n",
    "# dbutils.widgets.dropdown(\"dimension_table\", dimensions_list[0], dimensions_list)\n",
    "# dbutils.widgets.dropdown(\"scenario\", scenario_list[0], scenario_list)\n",
    "\n",
    "# selected_dimension = dbutils.widgets.get(\"dimension_table\")\n",
    "# selected_scenario = dbutils.widgets.get(\"scenario\")\n",
    "# print(f\"selected_dimension::{selected_dimension}\")\n",
    "# print(f\"selected_scenario::{selected_scenario}\")\n",
    "\n",
    "# job_id = None\n",
    "# if selected_dimension == \"D_GROUP\":\n",
    "#     job_id = d_group_job_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba69dd89-1223-4d0e-98bb-ddeab359a74e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3061bc5-8478-4d53-a279-8512a03007ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cleanup(p_catalog, p_schema, cleanup_table_list, cleanup_audit_table='Y', cleanup_runtime_config_table='Y'):\n",
    "    \n",
    "    if cleanup_runtime_config_table == 'Y':\n",
    "        print(f\"Deleting data from {p_catalog}.{p_schema}.{dlt_runtime_config_table}\")\n",
    "        spark.sql(f\"DELETE FROM {p_catalog}.{p_schema}.{dlt_runtime_config_table}\")\n",
    "\n",
    "    # cleanup audit table\n",
    "    if cleanup_audit_table == 'Y':\n",
    "        print(f\"Deleting data from {p_catalog}.{p_schema}.dlt_dq_audit\")\n",
    "        spark.sql(f\"DELETE FROM {p_catalog}.{p_schema}.dlt_dq_audit\")\n",
    "\n",
    "    # Drop all landing zone tables\n",
    "    # tables_df = spark.sql(f\"SHOW TABLES IN {p_catalog}.{p_schema}\").filter(\n",
    "    #     \"(tableName LIKE 'dlt_landing%') AND (tableName LIKE '%group%')\"\n",
    "    # )\n",
    "    for table in cleanup_table_list:\n",
    "        print(f\"cleanup_table_list::{cleanup_table_list}\")\n",
    "        # tables_df.select(\"tableName\").collect():\n",
    "        print(f\"Dropping table {p_catalog}.{p_schema}.{table}{table_suffix}\")\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {p_catalog}.{p_schema}.{table}{table_suffix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "515c306a-92ea-4f90-bb78-5759b3a31827",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### API: populate_runtime_metadata\n",
    "\n",
    "Populate the dim_group_runtime_metadata to test different scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "198ca069-db66-4680-86d3-7d279c85de99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def populate_runtime_metadata(\n",
    "    p_catalog,\n",
    "    p_schema,\n",
    "    p_dlt_runtime_config_table,\n",
    "    p_dimension_table,\n",
    "    p_scenario,\n",
    "    p_input_list,\n",
    "):\n",
    "    # Build sources_metadata part\n",
    "    sources_metadata_entries = []\n",
    "    for input in p_input_list:\n",
    "        scenario_input_source_system = input[\"scenario_input_source_system\"]\n",
    "        scenario_input_target_table = input[\"scenario_input_target_table\"]\n",
    "        scenario_input_business_date = input[\"scenario_input_business_date\"]\n",
    "        scenario_input_source_details = input[\"scenario_input_source_details\"]\n",
    "        scenario_input_format = input[\"scenario_input_format\"]\n",
    "        key = uuid.uuid4()\n",
    "        \n",
    "        sources_metadata_entries.append(\n",
    "            f\"'{key}', named_struct('business_date', '{scenario_input_business_date}', 'source_system', '{scenario_input_source_system}' , 'table_name', '{scenario_input_target_table}', 'source_details', '{scenario_input_source_details}','format', '{scenario_input_format}')\"\n",
    "        )\n",
    "\n",
    "    sources_metadata_map = \"map(\\n  \" + \",\\n  \".join(sources_metadata_entries) + \"\\n)\"\n",
    "\n",
    "    print(f\"populate_runtime_metadata::sources_metadata_entries::\\n{sources_metadata_entries}\")\n",
    "\n",
    "    # Final query\n",
    "    query = f\"\"\"\n",
    "      INSERT INTO {p_catalog}.{p_schema}.{p_dlt_runtime_config_table} (\n",
    "        dimension_table,\n",
    "        scenario,\n",
    "        run_date,\n",
    "        source_params\n",
    "      )\n",
    "      VALUES (\n",
    "        '{p_dimension_table}',\n",
    "        '{p_scenario}',\n",
    "        current_timestamp(),\n",
    "        {sources_metadata_map}\n",
    "      )\n",
    "    \"\"\"\n",
    "    spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e3fb1a4-8d01-4e0a-b21a-7fd5fbb8e790",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_full_refresh_selection(cdm_source):\n",
    "    source_map = {\n",
    "        \"DF_GROUP\": [\"table1\", \"table2\", \"table3\"],\n",
    "        \"DF_GROUP_HISTORY\": [\"table1\", \"table2\", \"table3\"],\n",
    "        \"DF_GROUP_COUNT\": [\"table6\", \"table7\", \"table8\", \"table9\"]\n",
    "    }\n",
    "    return source_map.get(cdm_source, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb1556ee-0e1a-495a-9e52-cbfec47d3645",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Trigger DLT pipeline API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "54a7801e-6add-407c-a4a0-36519ec1d058",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def run_pipeline(id):\n",
    "#     print(f\"Running pipeline {id}\")\n",
    "\n",
    "#     if cleanup == \"Y\":\n",
    "#         print(\"Cleaning up dim_group_runtime_metadata table\")\n",
    "#         spark.sql(f\"DELETE FROM {catalog}.{schema}.{dlt_runtime_config_table}\")\n",
    "\n",
    "#     populate_runtime_metadata(catalog, schema, dlt_runtime_config_table, scenario)\n",
    "\n",
    "#     if scenario == \"normal_run\" and business_date == \"N/A\":\n",
    "#         payload = {\n",
    "#             \"full_refresh\": True,\n",
    "#             \"cause\": f\"Triggered from ControlM simulator for scenario {scenario}\",\n",
    "#         }\n",
    "#     elif scenario == \"normal_run\" and business_date != \"N/A\":\n",
    "#         payload = {\n",
    "#             \"full_refresh\": False,\n",
    "#             \"cause\": f\"Triggered from ControlM simulator for scenario {scenario}\",\n",
    "#         }\n",
    "#     elif scenario == \"full_refresh\":\n",
    "#         payload = {\n",
    "#             \"full_refresh\": True,\n",
    "#             \"cause\": f\"Triggered from ControlM simulator for scenario {scenario}\",\n",
    "#         }\n",
    "#     elif scenario == \"full_refresh_selection\":\n",
    "#         payload = {\n",
    "#             \"full_refresh_selection\": get_full_refresh_selection(scenario),\n",
    "#             \"cause\": f\"Triggered from ControlM simulator for scenario {scenario}\",\n",
    "#         }\n",
    "#     else:\n",
    "#         raise Exception(f\"Invalid scenario: {scenario}\")\n",
    "\n",
    "#     response = requests.post(\n",
    "#         f\"{DATABRICKS_INSTANCE}/api/2.0/pipelines/{id}/updates\",\n",
    "#         headers={\n",
    "#             \"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\",\n",
    "#             \"Content-Type\": \"application/json\",\n",
    "#         },\n",
    "#         json=payload,\n",
    "#     )\n",
    "\n",
    "#     if response.status_code == 200:\n",
    "#         print(\n",
    "#             f\"message: Pipleline triggered successfully with update_id: {response.json()['update_id']}\"\n",
    "#         )\n",
    "#     else:\n",
    "#         print(f\"error: {response.text}, {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3af271f9-76ce-4974-852f-9f2a372121c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_job(\n",
    "    p_db_url,\n",
    "    p_api_token,\n",
    "    p_catalog,\n",
    "    p_schema,\n",
    "    p_dlt_runtime_config_table,\n",
    "    p_job_id,\n",
    "    selected_dimension,\n",
    "    selected_scenario,\n",
    "):\n",
    "    print(f\"run_job: Running job {p_job_id}\")\n",
    "\n",
    "    query = f\"\"\"\n",
    "    Select  \n",
    "    scenario_type, \n",
    "    refresh_type, \n",
    "    cleanup_table_list, \n",
    "    scenario_input_source_system, \n",
    "    scenario_input_target_table,\n",
    "    scenario_input_business_date,\n",
    "    scenario_input_source_details,\n",
    "    scenario_input_format \n",
    "    FROM  edl_dev_ctlg.structured.dim_integration_test_scenarios \n",
    "    WHERE dimension_table = '{selected_dimension}' and scenario = '{selected_scenario}'\"\"\"\n",
    "    \n",
    "    inputs = spark.sql(query).collect()\n",
    "    input_list = []\n",
    "    for input in inputs:\n",
    "        input_list.append(input)\n",
    "        scenario_type = input[\"scenario_type\"]\n",
    "        refresh_type = input[\"refresh_type\"]\n",
    "        cleanup_table_list = input[\"cleanup_table_list\"]        \n",
    "        # TODO handle proerly later\n",
    "        if cleanup_table_list is not None or cleanup_table_list is not []:           \n",
    "            cleanup(p_catalog, p_schema, cleanup_table_list)\n",
    "\n",
    "        populate_runtime_metadata(\n",
    "            p_catalog,\n",
    "            p_schema,\n",
    "            p_dlt_runtime_config_table,\n",
    "            selected_dimension,\n",
    "            scenario_type,\n",
    "            input_list,\n",
    "        )\n",
    "\n",
    "    payload = {\n",
    "        \"job_id\": p_job_id,\n",
    "        \"job_parameters\": {\n",
    "            \"dimension_table\": selected_dimension,\n",
    "            \"dlt_runtime_config_table\": dlt_runtime_config_table,\n",
    "            \"catalog\": p_catalog,\n",
    "            \"schema\": \"structured\",\n",
    "            \"scenario\": selected_scenario,\n",
    "            \"scenario_type\": scenario_type,\n",
    "            \"refresh_type\": refresh_type,\n",
    "            \"source_system\": \"\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        f\"{p_db_url}/api/2.1/jobs/run-now\",\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {p_api_token}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        },\n",
    "        json=payload,\n",
    "    )\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(\n",
    "            f\"run_job: message: Job triggered successfully with run_id: {response.json()['run_id']}\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"run_job: error: {response.text}, {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a08f2cf4-14c1-41c9-a12f-d4bee6df380c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run pipeline\n",
    "Multiple DLT pipelines cannot be triggered at the same time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61163ce1-2996-45b3-88e7-0c0ff163427b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_job: Running job 825821145657127\n",
      "dimension_table: D_GROUP\n",
      "  scenario: apply_correction\n",
      "  refresh_type: incremental_refresh\n",
      "  source_info: {'key1': {'business_date': '20250408', 'source_system': 'Facets', 'target_table': 'DF_GROUP'}} \n",
      " scenario_type: delta_correction\n",
      "populate_runtime_metadata::sources_metadata_entries::\n",
      "[\"'key1', named_struct('business_date', '20250408', 'source_system', 'Facets' , 'table_name', 'DF_GROUP', 'source_details', 'None','format', 'None')\"]\n",
      "run_job: message: Job triggered successfully with run_id: 587934046247698\n"
     ]
    }
   ],
   "source": [
    "run_job(\n",
    "    db_workspace_url,\n",
    "    DATABRICKS_TOKEN,\n",
    "    catalog,\n",
    "    bronze_schema,\n",
    "    dlt_runtime_config_table,\n",
    "    job_id,\n",
    "    selected_dimension,\n",
    "    selected_scenario\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2,
    "widgetLayout": []
   },
   "notebookName": "integration_test_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
